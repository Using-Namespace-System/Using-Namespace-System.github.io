<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Natural Language Processing[1]: Term-Document Matrix - Using-Namespace-System.github.io</title><meta name="description" content="The first in a series of posts on statistical language modeling. Installment includes corpus preprocessing, tokenization, and vectorization"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://using-namespace-system.github.io/natural-language-processing-1-term-document-matrix.html"><link rel="alternate" type="application/atom+xml" href="https://using-namespace-system.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://using-namespace-system.github.io/feed.json"><meta property="og:title" content="Natural Language Processing[1]: Term-Document Matrix"><meta property="og:image" content="https://using-namespace-system.github.io/media/website/Capture.PNG"><meta property="og:image:width" content="394"><meta property="og:image:height" content="222"><meta property="og:site_name" content="Using-Namespace-System.github.io"><meta property="og:description" content="The first in a series of posts on statistical language modeling. Installment includes corpus preprocessing, tokenization, and vectorization"><meta property="og:url" content="https://using-namespace-system.github.io/natural-language-processing-1-term-document-matrix.html"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://using-namespace-system.github.io/media/website/favicon-16x16.png" type="image/x-icon"><link rel="stylesheet" href="https://using-namespace-system.github.io/assets/css/style.css?v=6fbb1e8931a5afe843374fd67c192c86"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://using-namespace-system.github.io/natural-language-processing-1-term-document-matrix.html"},"headline":"Natural Language Processing[1]: Term-Document Matrix","datePublished":"2024-01-08T20:53","dateModified":"2024-01-08T21:03","image":{"@type":"ImageObject","url":"https://using-namespace-system.github.io/media/website/Capture.PNG","height":222,"width":394},"description":"The first in a series of posts on statistical language modeling. Installment includes corpus preprocessing, tokenization, and vectorization","author":{"@type":"Person","name":"Brian Recktenwall-Calvet","url":"https://using-namespace-system.github.io/authors/brian-recktenwall-calvet/"},"publisher":{"@type":"Organization","name":"Brian Recktenwall-Calvet","logo":{"@type":"ImageObject","url":"https://using-namespace-system.github.io/media/website/Capture.PNG","height":222,"width":394}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://using-namespace-system.github.io/"><img src="https://using-namespace-system.github.io/media/website/Capture.PNG" alt="Using-Namespace-System.github.io" width="394" height="222"></a><div class="search"><div class="search__overlay js-search-overlay"><div class="search__overlay-inner"><form action="https://using-namespace-system.github.io/search.html" class="search__form"><input class="search__input" type="search" name="q" placeholder="search..." aria-label="search..."></form><button class="search__close js-search-close" aria-label="Close">Close</button></div></div><button class="search__btn js-search-btn" aria-label="Search"><svg role="presentation" focusable="false"><use xlink:href="https://using-namespace-system.github.io/assets/svg/svg-map.svg#search"/></svg></button></div></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="https://using-namespace-system.github.io/media/website/computer-texture-medium-8.png" srcset="https://using-namespace-system.github.io/media/website/responsive/computer-texture-medium-8-xs.png 300w, https://using-namespace-system.github.io/media/website/responsive/computer-texture-medium-8-sm.png 480w, https://using-namespace-system.github.io/media/website/responsive/computer-texture-medium-8-md.png 768w, https://using-namespace-system.github.io/media/website/responsive/computer-texture-medium-8-lg.png 1024w, https://using-namespace-system.github.io/media/website/responsive/computer-texture-medium-8-xl.png 1360w, https://using-namespace-system.github.io/media/website/responsive/computer-texture-medium-8-2xl.png 1600w" sizes="100vw" loading="eager" alt=""></figure><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2024-01-08T20:53">January 8, 2024</time></div><h1>Natural Language Processing[1]: Term-Document Matrix</h1><div class="post__meta post__meta--author"><img src="https://using-namespace-system.github.io/media/website/15972706_10209572272237582_6617888217074726293_o.jpg" loading="eager" height="2048" width="1796" class="post__author-thumb" alt="Brian Recktenwall-Calvet"> <a href="https://using-namespace-system.github.io/authors/brian-recktenwall-calvet/" class="feed__author">Brian Recktenwall-Calvet</a></div></div></header></div><div class="wrapper post__entry"><main><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">This is the first in a series of posts on extracting word representations using statistical language modeling techniques. This first installment includes rudimentary corpus preprocessing, tokenization, and vectorization. The corpus is a public domain dataset of a million news headlines from the Australian Broadcasting Corporation between 2003 and 2021.</span></div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><p>All code blocks are included in this document and the first block includes the imports used in this project.</p></div></div></div></div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div><div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline"><div class="cm-editor cm-s-jupyter"><div class="highlight hl-ipython3"><pre><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">zip_longest</span>
<span class="kn">from</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">import</span> <span class="n">figure</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">csr_array</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">find</span>
<span class="kn">from</span> <span class="nn">pickleshare</span> <span class="kn">import</span> <span class="n">PickleShareDB</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'../input/abcnews-date-text.csv'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'stopwords'</span><span class="p">)</span>
<span class="n">stopwords_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">'english'</span><span class="p">))</span>
</pre></div></div></div></div></div></div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt"> </div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><p>Preprocessing the corpus is simplified to filtering out short headlines, small words, and stop-words. Each action is completed in pandas, I believe this may improve readability. The documents are exploded into a single series representing the whole corpus from here stop-words can be filtered out. No further sanitation is performed.</p></div></div></div></div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div><div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline"><div class="cm-editor cm-s-jupyter"><div class="highlight hl-ipython3"><pre><span class="c1">#tokenize and sanitize</span>

<span class="c1">#tokenize documents into individual words</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'tokenized'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">headline_text</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>

<span class="c1">#remove short documents from corpus</span>
<span class="n">df</span><span class="p">[</span><span class="s1">'length'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="nb">len</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1">#use random subset of corpus</span>
<span class="c1">#df=df.sample(frac=0.0016).reset_index()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>

<span class="c1">#flatten all words into single series</span>
<span class="n">ex</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">explode</span><span class="p">(</span><span class="s1">'tokenized'</span><span class="p">)</span>

<span class="c1">#remove shorter words</span>
<span class="n">ex</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ex</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>

<span class="c1">#remove stop-words</span>
<span class="n">ex</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">ex</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">stopwords_set</span><span class="p">)]</span>
</pre></div></div></div></div></div></div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt"> </div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><p>Tokenization of the corpus is performed by creating forward and backwards lookup dictionaries. Each unique word is represented as a unique number. This is a very simple method of tokenization.</p></div></div></div></div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div><div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline"><div class="cm-editor cm-s-jupyter"><div class="highlight hl-ipython3"><pre><span class="c1">#create dictionary of words</span>

<span class="c1">#shuffle for sparse matrix visual</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1">#dataframe with (index/code):word</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">name</span><span class="o">=</span><span class="s1">'words'</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>

<span class="c1">#store code:word dictionary for reverse encoding</span>
<span class="n">dictionary_lookup</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()[</span><span class="s1">'words'</span><span class="p">]</span>

<span class="c1">#offset index to prevent clash with zero fill</span>
<span class="n">dictionary</span><span class="p">[</span><span class="s1">'encode'</span><span class="p">]</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1">#store word:code dictionary for encoding</span>
<span class="n">dictionary</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">'words'</span><span class="p">)</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()[</span><span class="s1">'encode'</span><span class="p">]</span>

<span class="c1">#use dictionary to encode each word to integer representation</span>
<span class="n">encode</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">dictionary</span><span class="o">.</span><span class="n">get</span><span class="p">)</span><span class="o">.</span><span class="n">to_frame</span><span class="p">()</span>
<span class="n">encode</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'int'</span><span class="p">)</span>
<span class="n">encode</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'int'</span><span class="p">)</span>
<span class="c1">#un-flatten encoded words back into original documents</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">encode</span><span class="o">.</span><span class="n">tokenized</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="nb">tuple</span><span class="p">)</span>

<span class="c1">#match up document indexes for reverse lookup</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">docs</span><span class="o">.</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()[</span><span class="s1">'tokenized'</span><span class="p">]</span>
</pre></div></div></div></div></div></div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt"> </div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><p>Vectorization here is done in its simplest form. The word vector for each term is the one-hot encoding of the documents they are and are not present in. The transform is comprised of document-word vectors where each is a one-hot encoding of the terms in the corpus that are and are not present in a document.</p></div></div></div></div><div class="jp-Cell jp-CodeCell jp-Notebook-cell jp-mod-noOutputs"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div><div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline"><div class="cm-editor cm-s-jupyter"><div class="highlight hl-ipython3"><pre><span class="c1">#zero pad x dimension by longest sentence</span>
<span class="n">encoded_docs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="n">docs</span><span class="o">.</span><span class="n">to_list</span><span class="p">(),</span> <span class="n">fillvalue</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span>

<span class="c1">#convert to sparse matrix</span>
<span class="n">encoded_docs</span> <span class="o">=</span> <span class="n">csr_array</span><span class="p">(</span><span class="n">encoded_docs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

<span class="c1">#convert to index for each word</span>
<span class="n">row_column_code</span> <span class="o">=</span> <span class="n">find</span><span class="p">(</span><span class="n">encoded_docs</span><span class="p">)</span>

<span class="c1">#presort by words</span>
<span class="n">word_sorted_index</span> <span class="o">=</span> <span class="n">row_column_code</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
<span class="n">doc_word</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">row_column_code</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">word_sorted_index</span><span class="p">],</span> <span class="n">row_column_code</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">word_sorted_index</span><span class="p">]])</span>

<span class="c1">#presort by docs and words</span>
<span class="n">doc_word_sorted_index</span> <span class="o">=</span> <span class="n">doc_word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
<span class="n">doc_word</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">doc_word</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">doc_word_sorted_index</span><span class="p">],</span> <span class="n">doc_word</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">doc_word_sorted_index</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'doc'</span><span class="p">,</span><span class="s1">'word'</span><span class="p">])</span>

<span class="c1">#offset code no longer needed after zero-fill</span>
<span class="n">doc_word</span><span class="o">.</span><span class="n">word</span> <span class="o">=</span> <span class="n">doc_word</span><span class="o">.</span><span class="n">word</span> <span class="o">-</span> <span class="mi">1</span>

<span class="c1">#convert to index of word counts per document</span>
<span class="n">doc_word_count</span>  <span class="o">=</span> <span class="n">doc_word</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">'doc'</span><span class="p">,</span><span class="s1">'word'</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">to_frame</span><span class="p">(</span><span class="s1">'count'</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>

<span class="c1">#convert to sparse matrix</span>
<span class="n">sparse_word_doc_matrix</span> <span class="o">=</span> <span class="n">csr_array</span><span class="p">((</span><span class="n">doc_word_count</span><span class="p">[</span><span class="mi">2</span><span class="p">],(</span><span class="n">doc_word_count</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">doc_word_count</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">encoded_docs</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="c1">#visualize sparse matrix</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">sparse_word_doc_matrix_visualization</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sparse_word_doc_matrix_visualization</span><span class="o">.</span><span class="n">spy</span><span class="p">(</span><span class="n">sparse_word_doc_matrix</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mf">0.007</span><span class="p">,</span> <span class="n">aspect</span> <span class="o">=</span> <span class="s1">'auto'</span><span class="p">)</span>

<span class="o">%</span><span class="k">store</span> sparse_word_doc_matrix
<span class="o">%</span><span class="k">store</span> dictionary
<span class="o">%</span><span class="k">store</span> dictionary_lookup
</pre></div></div></div></div></div></div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt"> </div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><p>The visualization below shows the words (y-axis) and the documents (x-axis) they are in.</p></div></div></div></div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt"> </div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><figure class="post__image"><img loading="lazy" src="https://using-namespace-system.github.io/media/posts/2/sparse_word_doc_matrix.png" alt="" width="1244" height="1221" sizes="100vw" srcset="https://using-namespace-system.github.io/media/posts/2/responsive/sparse_word_doc_matrix-xs.png 300w, https://using-namespace-system.github.io/media/posts/2/responsive/sparse_word_doc_matrix-sm.png 480w, https://using-namespace-system.github.io/media/posts/2/responsive/sparse_word_doc_matrix-md.png 768w, https://using-namespace-system.github.io/media/posts/2/responsive/sparse_word_doc_matrix-lg.png 1024w, https://using-namespace-system.github.io/media/posts/2/responsive/sparse_word_doc_matrix-xl.png 1360w, https://using-namespace-system.github.io/media/posts/2/responsive/sparse_word_doc_matrix-2xl.png 1600w"></figure></div></div></div></div><div class="jp-Cell jp-MarkdownCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt"> </div><div class="jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput" data-mime-type="text/markdown"><p>The words that occur together often in the corpus also, as word vectors, are closer together in this 1200000 dimensional vector space. This is demonstrated in the table below.</p></div></div></div></div><div class="jp-Cell jp-CodeCell jp-Notebook-cell"><div class="jp-Cell-inputWrapper" tabindex="0"><div class="jp-Collapser jp-InputCollapser jp-Cell-inputCollapser"> </div><div class="jp-InputArea jp-Cell-inputArea"><div class="jp-InputPrompt jp-InputArea-prompt">In [ ]:</div><div class="jp-CodeMirrorEditor jp-Editor jp-InputArea-editor" data-type="inline"><div class="cm-editor cm-s-jupyter"><div class="highlight hl-ipython3"><pre><span class="c1">#approximating cosine similarity with dot product of the term document matrix and its transform</span>

<span class="n">similarity_matrix</span>  <span class="o">=</span> <span class="n">sparse_word_doc_matrix</span> <span class="o">@</span> <span class="n">sparse_word_doc_matrix</span><span class="o">.</span><span class="n">T</span>

<span class="c1">#displaying slice of matrix with highest similarity scores</span>

<span class="n">similarity_matrix_compressed</span> <span class="o">=</span> <span class="n">similarity_matrix</span><span class="p">[(</span><span class="o">-</span><span class="n">similarity_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="mi">20</span><span class="p">]]</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">((</span><span class="o">-</span><span class="n">similarity_matrix_compressed</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)[:</span><span class="mi">20</span><span class="p">,:</span><span class="mi">20</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="n">dictionary_lookup</span><span class="o">.</span><span class="n">get</span><span class="p">)</span>
</pre></div></div></div></div></div><div class="jp-Cell-outputWrapper"><div class="jp-Collapser jp-OutputCollapser jp-Cell-outputCollapser"> </div><div class="jp-OutputArea jp-Cell-outputArea"><div class="jp-OutputArea-child jp-OutputArea-executeResult"><div class="jp-OutputPrompt jp-OutputArea-prompt">Out[ ]:</div><div class="jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output jp-OutputArea-executeResult" tabindex="0" data-mime-type="text/html"><div><table class="dataframe" border="1"><thead><tr style="text-align: right;"><th> </th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th><th>17</th><th>18</th><th>19</th></tr></thead><tbody><tr><th>0</th><td>police</td><td>new</td><td>man</td><td>says</td><td>court</td><td>nsw</td><td>australia</td><td>govt</td><td>council</td><td>fire</td><td>australian</td><td>qld</td><td>sydney</td><td>plan</td><td>death</td><td>water</td><td>health</td><td>crash</td><td>back</td><td>coast</td></tr><tr><th>1</th><td>man</td><td>zealand</td><td>charged</td><td>govt</td><td>man</td><td>police</td><td>day</td><td>urged</td><td>plan</td><td>house</td><td>open</td><td>north</td><td>man</td><td>council</td><td>police</td><td>plan</td><td>mental</td><td>fatal</td><td>hits</td><td>gold</td></tr><tr><th>2</th><td>investigate</td><td>laws</td><td>police</td><td>minister</td><td>accused</td><td>rural</td><td>south</td><td>nsw</td><td>new</td><td>crews</td><td>market</td><td>govt</td><td>police</td><td>water</td><td>man</td><td>restrictions</td><td>service</td><td>car</td><td>bounce</td><td>north</td></tr><tr><th>3</th><td>probe</td><td>police</td><td>court</td><td>trump</td><td>face</td><td>govt</td><td>coronavirus</td><td>qld</td><td>considers</td><td>police</td><td>dollar</td><td>rural</td><td>hobart</td><td>govt</td><td>toll</td><td>council</td><td>minister</td><td>plane</td><td>fight</td><td>sunshine</td></tr><tr><th>4</th><td>missing</td><td>cases</td><td>murder</td><td>new</td><td>told</td><td>country</td><td>new</td><td>vic</td><td>water</td><td>man</td><td>south</td><td>central</td><td>western</td><td>new</td><td>inquest</td><td>supply</td><td>new</td><td>dies</td><td>track</td><td>nsw</td></tr><tr><th>5</th><td>search</td><td>australia</td><td>jailed</td><td>australia</td><td>faces</td><td>hour</td><td>live</td><td>says</td><td>land</td><td>threat</td><td>year</td><td>police</td><td>charged</td><td>basin</td><td>charged</td><td>govt</td><td>services</td><td>killed</td><td>plan</td><td>south</td></tr><tr><th>6</th><td>car</td><td>york</td><td>dies</td><td>labor</td><td>murder</td><td>coast</td><td>test</td><td>plan</td><td>seeks</td><td>govt</td><td>share</td><td>health</td><td>nsw</td><td>backs</td><td>court</td><td>murray</td><td>qld</td><td>police</td><td>court</td><td>west</td></tr><tr><th>7</th><td>death</td><td>council</td><td>missing</td><td>union</td><td>high</td><td>new</td><td>india</td><td>local</td><td>says</td><td>destroys</td><td>new</td><td>new</td><td>morning</td><td>murray</td><td>rises</td><td>new</td><td>indigenous</td><td>man</td><td>urged</td><td>police</td></tr><tr><th>8</th><td>officer</td><td>year</td><td>accused</td><td>could</td><td>front</td><td>coronavirus</td><td>world</td><td>new</td><td>plans</td><td>nsw</td><td>china</td><td>government</td><td>briefing</td><td>group</td><td>probe</td><td>use</td><td>funding</td><td>driver</td><td>get</td><td>mid</td></tr><tr><th>9</th><td>hunt</td><td>nsw</td><td>guilty</td><td>government</td><td>hears</td><td>government</td><td>cup</td><td>fed</td><td>city</td><td>school</td><td>first</td><td>south</td><td>airport</td><td>says</td><td>woman</td><td>irrigators</td><td>says</td><td>road</td><td>says</td><td>man</td></tr><tr><th>10</th><td>crash</td><td>coronavirus</td><td>car</td><td>council</td><td>case</td><td>covid</td><td>says</td><td>funding</td><td>backs</td><td>suspicious</td><td>shares</td><td>election</td><td>fire</td><td>housing</td><td>coroner</td><td>says</td><td>system</td><td>two</td><td>govt</td><td>central</td></tr><tr><th>11</th><td>arrest</td><td>south</td><td>arrested</td><td>opposition</td><td>charges</td><td>north</td><td>cricket</td><td>funds</td><td>rejects</td><td>factory</td><td>man</td><td>country</td><td>new</td><td>health</td><td>investigate</td><td>nsw</td><td>govt</td><td>highway</td><td>australia</td><td>east</td></tr><tr><th>12</th><td>murder</td><td>hospital</td><td>death</td><td>report</td><td>supreme</td><td>cases</td><td>china</td><td>act</td><td>urged</td><td>service</td><td>says</td><td>western</td><td>harbour</td><td>government</td><td>suspicious</td><td>residents</td><td>workers</td><td>injured</td><td>work</td><td>cyclone</td></tr><tr><th>13</th><td>shooting</td><td>centre</td><td>crash</td><td>wont</td><td>alleged</td><td>sydney</td><td>covid</td><td>defends</td><td>rise</td><td>home</td><td>wins</td><td>water</td><td>shooting</td><td>public</td><td>penalty</td><td>urged</td><td>hospital</td><td>dead</td><td>win</td><td>council</td></tr><tr><th>14</th><td>say</td><td>years</td><td>stabbing</td><td>group</td><td>appeal</td><td>premier</td><td>first</td><td>water</td><td>rates</td><td>sydney</td><td>chinese</td><td>outback</td><td>south</td><td>rejects</td><td>baby</td><td>qld</td><td>care</td><td>truck</td><td>new</td><td>qld</td></tr><tr><th>15</th><td>seek</td><td>says</td><td>found</td><td>expert</td><td>police</td><td>drought</td><td>western</td><td>accused</td><td>mayor</td><td>residents</td><td>team</td><td>says</td><td>melbourne</td><td>back</td><td>murder</td><td>farmers</td><td>nsw</td><td>bus</td><td>school</td><td>new</td></tr><tr><th>16</th><td>nsw</td><td>home</td><td>attack</td><td>must</td><td>sex</td><td>fire</td><td>zealand</td><td>federal</td><td>rate</td><td>season</td><td>killed</td><td>man</td><td>league</td><td>opposition</td><td>custody</td><td>pipeline</td><td>rural</td><td>woman</td><td>police</td><td>sun</td></tr><tr><th>17</th><td>fatal</td><td>govt</td><td>assault</td><td>health</td><td>death</td><td>election</td><td>news</td><td>fire</td><td>development</td><td>new</td><td>government</td><td>hour</td><td>woman</td><td>management</td><td>guilty</td><td>price</td><td>boost</td><td>probe</td><td>bring</td><td>found</td></tr><tr><th>18</th><td>new</td><td>deal</td><td>child</td><td>police</td><td>woman</td><td>farmers</td><td>england</td><td>rejects</td><td>budget</td><td>ban</td><td>day</td><td>far</td><td>siege</td><td>labor</td><td>family</td><td>bans</td><td>report</td><td>pilot</td><td>hit</td><td>afl</td></tr><tr><th>19</th><td>drug</td><td>covid</td><td>killed</td><td>chief</td><td>drug</td><td>west</td><td>one</td><td>boost</td><td>govt</td><td>danger</td><td>coronavirus</td><td>coast</td><td>found</td><td>nsw</td><td>driver</td><td>boost</td><td>plan</td><td>victim</td><td>pay</td><td>missing</td></tr></tbody></table></div></div></div></div></div></div></main></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on January 8, 2024</p><div class="post__share"></div><div class="post__bio bio"><img src="https://using-namespace-system.github.io/media/website/15972706_10209572272237582_6617888217074726293_o.jpg" loading="lazy" height="2048" width="1796" class="bio__avatar" alt="Brian Recktenwall-Calvet"><div><h3 class="bio__name"><a href="https://using-namespace-system.github.io/authors/brian-recktenwall-calvet/" rel="author">Brian Recktenwall-Calvet</a></h3><div class="bio__desc"><p>Cats and Qubits</p></div></div></div></footer></article></main><footer class="footer"><div class="footer__copyright"><p><a href="Using-Namespace-System.github.io">Using-Namespace-System.github.io</a></p></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://using-namespace-system.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script defer="defer" src="https://using-namespace-system.github.io/assets/js/scripts.min.js?v=f47c11534595205f20935f0db2a62a85"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>